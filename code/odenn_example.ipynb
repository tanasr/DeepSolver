{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ODE Neural Network approximator\n",
    "# code from https://colab.research.google.com/drive/12ztGwxR1TK8Ka6H3bOsSt57kB71ieQ-W?usp=sharing#scrollTo=5M126csrMRHM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial condition\n",
    "f0 = 1\n",
    "# infinitesimal small number\n",
    "inf_s = np.sqrt(np.finfo(np.float32).eps)\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_steps = 1000\n",
    "batch_size = 100\n",
    "display_step = training_steps/10\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 1     # input layer number of neurons\n",
    "n_hidden_1 = 32 # 1st layer number of neurons\n",
    "n_hidden_2 = 32 # 2nd layer number of neurons\n",
    "n_output = 1    # output layer number of neurons\n",
    "\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random.normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random.normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random.normal([n_hidden_2, n_output]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random.normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random.normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random.normal([n_output]))\n",
    "}\n",
    "\n",
    "# Stochastic gradient descent optimizer.\n",
    "optimizer = tf.optimizers.SGD(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "def multilayer_perceptron(x):\n",
    "    x = np.array([[[x]]],  dtype='float32')\n",
    "    # Hidden fully connected layer with 32 neurons\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.sigmoid(layer_1)\n",
    "    # Hidden fully connected layer with 32 neurons\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.sigmoid(layer_2)\n",
    "    # Output fully connected layer\n",
    "    output = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return tf.nn.sigmoid(output)\n",
    "\n",
    "# Universal Approximator\n",
    "def g(x):\n",
    "    return x * multilayer_perceptron(x) + f0\n",
    "\n",
    "# Given EDO\n",
    "def f(x):\n",
    "    return 2*x\n",
    "    \n",
    "# Custom loss function to approximate the derivatives\n",
    "def custom_loss():\n",
    "    summation = []\n",
    "    for x in np.linspace(0,1,10):\n",
    "        dNN = (g(x+inf_s)-g(x))/inf_s\n",
    "        summation.append((dNN - f(x))**2)\n",
    "    return tf.reduce_sum(tf.abs(summation))\n",
    "    # return tf.sqrt(tf.reduce_mean(tf.abs(summation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step():\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = custom_loss()\n",
    "    trainable_variables = list(weights.values()) + list(biases.values())\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(training_steps):\n",
    "    train_step()\n",
    "    if i % display_step == 0:\n",
    "        print(\"loss: %f \" % (custom_loss()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(figsize=(10,10))\n",
    "# True Solution (found analitically)\n",
    "def true_solution(x):\n",
    "    return x**2 + 1\n",
    "\n",
    "X = np.linspace(0, 1, 100)\n",
    "result = []\n",
    "for i in X:\n",
    "  # result.append(f(i))\n",
    "  result.append(g(i).numpy()[0][0][0])\n",
    "\n",
    "S = true_solution(X)\n",
    "  \n",
    "plt.plot(X, S, label=\"Original Function\")\n",
    "plt.plot(X, result, label=\"Neural Net Approximation\")\n",
    "plt.legend(loc=2, prop={'size': 20})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The same code, but now we are using tf.GradientTape to automatically take the derivative for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'h1': tf.Variable(tf.random.normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random.normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random.normal([n_hidden_2, n_output]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random.normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random.normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random.normal([n_output]))\n",
    "}\n",
    "\n",
    "def train_step_t():\n",
    "    X = np.linspace(0,1,10)\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        summation = []\n",
    "        for x in X: \n",
    "            x = tf.constant([[[x]]],  dtype='float32')\n",
    "            tape.watch(x)\n",
    "            g_x = x * multilayer_perceptron(x) + f0\n",
    "            dNN = tape.gradient(g_x, x)\n",
    "            summation.append(dNN - f(x))\n",
    "        loss = tf.reduce_sum(tf.abs(summation))\n",
    "    trainable_variables = list(weights.values()) + list(biases.values())\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "for i in range(training_steps):\n",
    "    train_step_t()\n",
    "    if i % display_step == 0:\n",
    "        print(\"loss: %f \" % (custom_loss()))\n",
    "\n",
    "figure(figsize=(10,10))\n",
    "# True Solution (found analitically)\n",
    "def true_solution(x):\n",
    "    return x**2 + 1\n",
    "\n",
    "X = np.linspace(0, 1, 100)\n",
    "result = []\n",
    "result2 = []\n",
    "for i in X:\n",
    "  # result.append(f(i))\n",
    "  result.append(g(i).numpy()[0][0][0])\n",
    "  \n",
    "\n",
    "S = true_solution(X)\n",
    "\n",
    "plt.plot(X, S, label=\"Original Function\")\n",
    "plt.plot(X, result, label=\"Neural Net Approximation\")\n",
    "plt.legend(loc=2, prop={'size': 20})\n",
    "plt.show()"
   ]
  }
 ]
}