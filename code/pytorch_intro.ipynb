{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/biaslyai/learn-pytorch-basics-6d433f186b7a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tensor\n",
    "new_tensor = torch.Tensor([[1, 2], [3, 4]])\n",
    "# create a 2 x 3 tensor with random values\n",
    "empty_tensor = torch.Tensor(2, 3)\n",
    "# create a 2 x 3 tensor with random values between -1and 1\n",
    "uniform_tensor = torch.Tensor(2, 3).uniform_(-1, 1)\n",
    "# create a 2 x 3 tensor with random values from a uniform distribution on the interval [0, 1)\n",
    "rand_tensor = torch.rand(2, 3)\n",
    "# create a 2 x 3 tensor of zeros\n",
    "zero_tensor = torch.zeros(2, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[5., 2.],\n        [3., 4.]])\ntensor(3.)\n3.0\ntensor([1., 4., 7.])\ntensor([3., 6., 9.])\ntensor([7., 8., 9.])\ntensor([[1., 2., 3.],\n        [4., 5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "new_tensor = torch.Tensor([[1, 2], [3, 4]])\n",
    "# replace an element at position 0, 0\n",
    "new_tensor[0][0] = 5\n",
    "print(new_tensor)                 # tensor([[ 5.,  2.],[ 3.,  4.]])\n",
    "# access an element at position 1, 0\n",
    "print(new_tensor[1][0])           # tensor([ 3.])\n",
    "print(new_tensor[1][0].item())    # 3.\n",
    "## slicing examples\n",
    "slice_tensor = torch.Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "# elements from every row, first column\n",
    "print(slice_tensor[:, 0])         # tensor([ 1.,  4.,  7.])\n",
    "# elements from every row, last column\n",
    "print(slice_tensor[:, -1])        # tensor([ 3.,  6.,  9.])\n",
    "# all elements on the second row\n",
    "print(slice_tensor[2, :])         # tensor([ 4.,  5.,  6.])\n",
    "# all elements from first two rows\n",
    "print(slice_tensor[:2, :])        # tensor([[ 1.,  2.,  3.],[ 4.,  5.,  6.]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.FloatTensor\ntorch.Size([2, 2])\ntorch.Size([2, 2])\n2\n"
     ]
    }
   ],
   "source": [
    "new_tensor = torch.Tensor([[1, 2], [3, 4]])\n",
    "# type of a tensor\n",
    "print(new_tensor.type())   # 'torch.FloatTensor'\n",
    "# shape of a tensor\n",
    "print(new_tensor.shape)    # torch.Size([2, 2])\n",
    "print(new_tensor.size())   # torch.Size([2, 2])\n",
    "# dimension of a tensor\n",
    "print(new_tensor.dim())    # 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [2.],\n",
       "        [3.],\n",
       "        [4.]])"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "reshape_tensor = torch.Tensor([[1, 2], [3, 4]])\n",
    "reshape_tensor.view(1,4)   # tensor([[ 1.,  2.,  3.,  4.]])\n",
    "reshape_tensor.view(4,1)   # tensor([[ 1.],[ 2.],[ 3.],[ 4.]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_ndarray = np.random.randn(2,2)\n",
    "# NumPy ndarray to PyTorch tensor\n",
    "to_tensor = torch.from_numpy(np_ndarray)\n",
    "# PyTorch tensor to NumPy array\n",
    "to_ndarray = to_tensor.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-1.6477,  0.1097, -0.9960],\n",
       "        [-1.1885,  0.7446,  1.6984],\n",
       "        [-0.5316,  1.0670, -0.8024]])"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "tensor_1 = torch.randn(3, 3)\n",
    "tensor_2 = torch.randn(3, 3)\n",
    "\n",
    "cross_prod = tensor_1.cross(tensor_2)\n",
    "\n",
    "# regular transpose function\n",
    "tensor_1.t()\n",
    "# transpose via permute function\n",
    "tensor_2.permute(-1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxtrix_prod = tensor_1.mm(tensor_2)\n",
    "element_mult = tensor_1.mul(tensor_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    tensor_1 = tensor_1.cuda()\n",
    "    tensor_2 = tensor_2.cuda()\n",
    "    tensor_1 + tensor_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression using PyTorch\n",
    "# https://medium.com/biaslyai/pytorch-linear-and-logistic-regression-models-5c5f0da2cb9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = Variable(torch.Tensor([[10.0], [9.0], [3.0], [2.0]]))\n",
    "y_data = Variable(torch.Tensor([[90.0], [80.0], [50.0], [30.0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(1, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred\n",
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss(size_average=False)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    y_pred = model(x_data)\n",
    "\n",
    "    # Compute Loss\n",
    "    loss = criterion(y_pred, y_data)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "predicted Y value:  tensor(-8.8822e+10)\n"
     ]
    }
   ],
   "source": [
    "new_x = Variable(torch.Tensor([[4.0]]))\n",
    "y_pred = model(new_x)\n",
    "print(\"predicted Y value: \", y_pred.data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "class LogisticRegression(torch.nn.Module):\n",
    "     def __init__(self):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(1, 1)\n",
    "\n",
    "     def forward(self, x):\n",
    "        y_pred = F.sigmoid(self.linear(x))\n",
    "        return y_pred\n",
    "\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss(size_average=True)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    y_pred = model(x_data)\n",
    "\n",
    "    # Compute Loss\n",
    "    loss = criterion(y_pred, y_data)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "predicted Y value:  tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "new_x = Variable(torch.Tensor([[4.0]]))\n",
    "y_pred = model(new_x)\n",
    "print(\"predicted Y value: \", y_pred.data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed Forward NN\n",
    "# https://medium.com/biaslyai/pytorch-introduction-to-neural-network-feedforward-neural-network-model-e7231cff47cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(1, requires_grad=True)\n",
    "print(x.grad)    # returns None since x is a scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([12.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(1, requires_grad=True)\n",
    "y = x + 2\n",
    "z = 2*y**2\n",
    "\n",
    "z.backward()     # performs backward propagation automatically and calculates the gradient\n",
    "print(x.grad)    # ∂z/∂x = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Perceptron, self).__init__()\n",
    "        self.fc = nn.Linear(1,1)\n",
    "        self.relu = torch.nn.ReLU() # instead of Heaviside step fn\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.fc(x)\n",
    "        output = self.relu(x) # instead of Heaviside step fn\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feedforward(torch.nn.Module):\n",
    "        def __init__(self, input_size, hidden_size):\n",
    "            super(Feedforward, self).__init__()\n",
    "            self.input_size = input_size\n",
    "            self.hidden_size  = hidden_size\n",
    "            self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
    "            self.relu = torch.nn.ReLU()\n",
    "            self.fc2 = torch.nn.Linear(self.hidden_size, 1)\n",
    "            self.sigmoid = torch.nn.Sigmoid()\n",
    "            \n",
    "        def forward(self, x):\n",
    "            hidden = self.fc1(x)\n",
    "            relu = self.relu(hidden)\n",
    "            output = self.fc2(relu)\n",
    "            output = self.sigmoid(output)\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE RANDOM DATA POINTS\n",
    "from sklearn.datasets import make_blobs\n",
    "def blob_label(y, label, loc): # assign labels\n",
    "    target = np.copy(y)\n",
    "    for l in loc:\n",
    "        target[y == l] = label\n",
    "    return target\n",
    "x_train, y_train = make_blobs(n_samples=40, n_features=2, cluster_std=1.5, shuffle=True)\n",
    "x_train = torch.FloatTensor(x_train)\n",
    "y_train = torch.FloatTensor(blob_label(y_train, 0, [0]))\n",
    "y_train = torch.FloatTensor(blob_label(y_train, 1, [1,2,3]))\n",
    "x_test, y_test = make_blobs(n_samples=10, n_features=2, cluster_std=1.5, shuffle=True)\n",
    "x_test = torch.FloatTensor(x_test)\n",
    "y_test = torch.FloatTensor(blob_label(y_test, 0, [0]))\n",
    "y_test = torch.FloatTensor(blob_label(y_test, 1, [1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Feedforward(2, 10)\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test loss before training 0.9190226793289185\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "y_pred = model(x_test)\n",
    "before_train = criterion(y_pred.squeeze(), y_test)\n",
    "print('Test loss before training' , before_train.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 0: train loss: 0.9796863794326782\nEpoch 1: train loss: 0.9480264782905579\nEpoch 2: train loss: 0.9216745495796204\nEpoch 3: train loss: 0.8997436761856079\nEpoch 4: train loss: 0.8814278841018677\nEpoch 5: train loss: 0.8659717440605164\nEpoch 6: train loss: 0.8527995347976685\nEpoch 7: train loss: 0.8413684964179993\nEpoch 8: train loss: 0.8313230276107788\nEpoch 9: train loss: 0.8223169445991516\nEpoch 10: train loss: 0.814171314239502\nEpoch 11: train loss: 0.806696891784668\nEpoch 12: train loss: 0.7997024655342102\nEpoch 13: train loss: 0.7931452989578247\nEpoch 14: train loss: 0.7869433164596558\nEpoch 15: train loss: 0.7810333967208862\nEpoch 16: train loss: 0.7753660678863525\nEpoch 17: train loss: 0.7698675990104675\nEpoch 18: train loss: 0.7645470499992371\nEpoch 19: train loss: 0.7593821287155151\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "epoch = 20\n",
    "for epoch in range(epoch):\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass\n",
    "    y_pred = model(x_train)\n",
    "    # Compute Loss\n",
    "    loss = criterion(y_pred.squeeze(), y_train)\n",
    "   \n",
    "    print('Epoch {}: train loss: {}'.format(epoch, loss.item()))\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test loss after Training 0.7938277721405029\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "y_pred = model(x_test)\n",
    "after_train = criterion(y_pred.squeeze(), y_test) \n",
    "print('Test loss after Training' , after_train.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a Neural Network using PyTorch\n",
    "# https://towardsdatascience.com/building-neural-network-using-pytorch-84f6e75f9a\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Inputs to hidden layer linear transformation\n",
    "        self.hidden = nn.Linear(784, 256)\n",
    "        # Output layer, 10 units - one for each digit\n",
    "        self.output = nn.Linear(256, 10)\n",
    "        \n",
    "        # Define sigmoid activation and softmax output \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass the input tensor through each of our operations\n",
    "        x = self.hidden(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.output(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Network(\n",
       "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "# Create the network and look at it's text representation\n",
    "model = Network()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sequential(\n  (0): Linear(in_features=784, out_features=128, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=128, out_features=64, bias=True)\n  (3): ReLU()\n  (4): Linear(in_features=64, out_features=10, bias=True)\n  (5): Softmax(dim=1)\n)\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters for our network\n",
    "input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10\n",
    "# Build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.Softmax(dim=1))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (output): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "# we can also pass an ordered dict to name the layers accordingly\n",
    "from collections import OrderedDict\n",
    "model = nn.Sequential(OrderedDict([\n",
    "                      ('fc1', nn.Linear(input_size, hidden_sizes[0])),\n",
    "                      ('relu1', nn.ReLU()),\n",
    "                      ('fc2', nn.Linear(hidden_sizes[0], hidden_sizes[1])),\n",
    "                      ('relu2', nn.ReLU()),\n",
    "                      ('output', nn.Linear(hidden_sizes[1], output_size)),\n",
    "                      ('softmax', nn.Softmax(dim=1))]))\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Linear(in_features=784, out_features=128, bias=True)\nLinear(in_features=784, out_features=128, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# now we can access the layers either by integer or their names\n",
    "print(model[0])\n",
    "print(model.fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a Neural Network using PyTorch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training loss after 0 epochs: 1.9385424915915612\n",
      "Training loss after 1 epochs: 0.8801150240305898\n",
      "Training loss after 2 epochs: 0.5423011705755933\n",
      "Training loss after 3 epochs: 0.4406377552573615\n",
      "Training loss after 4 epochs: 0.39171836059739085\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "# Define the loss\n",
    "criterion = nn.NLLLoss()\n",
    "# Optimizers require the parameters to optimize and a learning rate\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
    "epochs = 5\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "    \n",
    "        # Training pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(\"Training loss after {0} epochs: {1}\".format(e+1,running_loss/len(trainloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tutorial from https://www.freecodecamp.org/news/how-to-build-a-neural-network-with-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n",
      "9913344it [00:00, 20187034.62it/s]                             \n",
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "29696it [00:00, 10909525.41it/s]         \n",
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "1649664it [00:00, 18916215.75it/s]         \n",
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "5120it [00:00, 8607148.89it/s]          Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(6131, 6265)"
      ]
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "mnist = datasets.MNIST('./data', download=True)\n",
    "\n",
    "threes = mnist.data[(mnist.targets == 3)]/255.0\n",
    "sevens = mnist.data[(mnist.targets == 7)]/255.0\n",
    "\n",
    "len(threes), len(sevens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 251.565 248.518125\" width=\"251.565pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2022-03-03T15:51:34.689152</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.518125 \nL 251.565 248.518125 \nL 251.565 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 26.925 224.64 \nL 244.365 224.64 \nL 244.365 7.2 \nL 26.925 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p1272f20d13)\">\n    <image height=\"218\" id=\"image611d93b803\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"26.925\" xlink:href=\"data:image/png;base64,\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\" y=\"-6.64\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m0668ea4250\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.807857\" xlink:href=\"#m0668ea4250\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(27.626607 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"69.636429\" xlink:href=\"#m0668ea4250\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 5 -->\n      <g transform=\"translate(66.455179 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"108.465\" xlink:href=\"#m0668ea4250\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 10 -->\n      <g transform=\"translate(102.1025 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"147.293571\" xlink:href=\"#m0668ea4250\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 15 -->\n      <g transform=\"translate(140.931071 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"186.122143\" xlink:href=\"#m0668ea4250\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 20 -->\n      <g transform=\"translate(179.759643 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"224.950714\" xlink:href=\"#m0668ea4250\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 25 -->\n      <g transform=\"translate(218.588214 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m5fdd81b0e8\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m5fdd81b0e8\" y=\"11.082857\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0 -->\n      <g transform=\"translate(13.5625 14.882076)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m5fdd81b0e8\" y=\"49.911429\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 5 -->\n      <g transform=\"translate(13.5625 53.710647)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m5fdd81b0e8\" y=\"88.74\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 10 -->\n      <g transform=\"translate(7.2 92.539219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m5fdd81b0e8\" y=\"127.568571\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 15 -->\n      <g transform=\"translate(7.2 131.36779)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m5fdd81b0e8\" y=\"166.397143\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 20 -->\n      <g transform=\"translate(7.2 170.196362)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m5fdd81b0e8\" y=\"205.225714\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 25 -->\n      <g transform=\"translate(7.2 209.024933)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 26.925 224.64 \nL 26.925 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 244.365 224.64 \nL 244.365 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 26.925 224.64 \nL 244.365 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 26.925 7.2 \nL 244.365 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p1272f20d13\">\n   <rect height=\"217.44\" width=\"217.44\" x=\"26.925\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAOi0lEQVR4nO3df4wU93nH8c8D4TeGcHEgFOM6taAtdRNwrkBj1KayYjlWJOwmtUwlF7tuzqqMf7Q0teVUiqtGkdvESa26dXSJUbDl2I2UUCPVakNOIMuxgzgo4YdxgLj4BzqOuEQF7BqO4+kfN0QH3Hx32Znd2eN5v6TT7s6zM/No7Q+zO9+d/Zq7C8DFb0zVDQBoDcIOBEHYgSAIOxAEYQeCeF8rdzbeJvhETWnlLoFQ3tM7OuknbKRaobCb2fWSHpU0VtK33P3h1PMnaoqW2LVFdgkgYbP35NYafhtvZmMl/bOkT0laIGmFmS1odHsAmqvIZ/bFkva7+2vuflLSs5KWl9MWgLIVCfscSW8Oe/xWtuwsZtZlZr1m1jugEwV2B6CIpp+Nd/dud+90985xmtDs3QHIUSTsByXNHfb4smwZgDZUJOxbJM0zsw+b2XhJt0haX05bAMrW8NCbu58ys1WS/lNDQ29r3H13aZ0BKFWhcXZ3f17S8yX1AqCJ+LosEARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRSaxRWtMeaSS5L1tz97VW5t+V9sTK77N5e+mqwP+ulkvYhFj6xK1if8wgttf8ZP382t2Us/KbTt0ahQ2M3sgKRjkgYlnXL3zjKaAlC+Mo7sf+Dub5ewHQBNxGd2IIiiYXdJPzCzrWbWNdITzKzLzHrNrHdAJwruDkCjir6NX+buB81spqQNZvaqu78w/Anu3i2pW5KmWUexMy4AGlboyO7uB7Pbw5LWSVpcRlMAytdw2M1sipldcua+pOsk7SqrMQDlKvI2fpakdWZ2Zjvfcff/KKWrNmSd+WPZr99vTd331Enpcx0vLXqs4W0PVPjBauvqf2rq9v/xF/Nzaz0rlybX9a27y26ncg2H3d1fk/TREnsB0EQMvQFBEHYgCMIOBEHYgSAIOxCEubdu7GWadfgSu7Zl+yvTPfvzLwW9btI7LewEZXjj1P8l65/ceG+yPu+2rWW2U5rN3qOjfmTEsWCO7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBD8lXadVPbfm1vZ++htN3ffegZPJ+mfWrs6t3fdHzyXXvWP6Gw31NNpd/r5Jyfqfd25K1jf87rJk3V5uv5+q5sgOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0FwPXudUj8lbSdPNXffA4PJ+uCefbm1sQvyf05Zkl59YGqy/utfzZ/2WJI+9mT6J5dvmp5/3fdHxo9Nrlul3TX+m979l/ck65PXbS6znbpxPTsAwg5EQdiBIAg7EARhB4Ig7EAQhB0IguvZ69S/dFpubeZjL7Wwkwsz+MreZH3en6TXP11j+1tX5n//QJKOr5mQW/vKh6oZi5ak/sH078Z/7u8+n6x3rHu5zHZaouaR3czWmNlhM9s1bFmHmW0ws33Z7YzmtgmgqHrexn9b0vXnLHtAUo+7z5PUkz0G0MZqht3dX5B05JzFyyWtze6vlXRjuW0BKFujn9lnuXtfdv+QpFl5TzSzLkldkjRRkxvcHYCiCp+N96EraXKvpnH3bnfvdPfOcco/WQOguRoNe7+ZzZak7PZweS0BaIZGw75e0srs/kpJ6d8rBlC5mtezm9kzkj4h6VJJ/ZK+KOnfJH1X0uWSXpd0s7ufexLvPKP5evZ2Nvb903Nrftns5LofeSp/3vl6XDkx/abu9mlvFtp+Eak52D/793+dXHfmv7TvdydSUtez1zxB5+4rckqkFhhF+LosEARhB4Ig7EAQhB0IgrADQXCJ6yiQGlqTpP6ncr+trB9f/XTZ7bSNWlNZr0xcpjrzidE5tFYER3YgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIJx9tHggx9Iln989TMtaqS9/PGjq5P1DwUcS0/hyA4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQTDOPgrY8XeT9Qf7O3NrX57VW3Y7bePRVd9I1r/0X7fl1sZu2lZuM6MAR3YgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIJx9lHgVN+hZH3Xny7IrS1ZurTsds7S84VHkvWpYyY0bd/XTBxI1n92S/7/3vM3ldzMKFDzyG5ma8zssJntGrbsITM7aGbbs78bmtsmgKLqeRv/bUnXj7D86+6+MPt7vty2AJStZtjd/QVJR1rQC4AmKnKCbpWZ7cje5s/Ie5KZdZlZr5n1DuhEgd0BKKLRsD8u6UpJCyX1Sco9S+Pu3e7e6e6d49S8kzUA0hoKu7v3u/ugu5+W9E1Ji8ttC0DZGgq7mc0e9vAmSbvyngugPZi7p59g9oykT0i6VFK/pC9mjxdKckkHJN3p7n21djbNOnyJXVukX7SZMZMnJ+v7vjU/t7bn958ou52znPD8cfhP33l3ct0J/76l7HZaYrP36KgfsZFqNb9U4+4rRljc3P9KAErH12WBIAg7EARhB4Ig7EAQhB0IgktcUcjpd9M/c/0r/zo+t7b34yeT684fl79uPSbYuNya24ijUxc1juxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EATj7Giqqfv/N7d27HSxcfRafmPjn+XW5v9wR3Ld02U30wY4sgNBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIyzoxBb9FvJ+sI1O3NrH2vyBEGnj+dfz376vfeau/M2xJEdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4IIM87+zmeWJOt/+Lcbmrbv9fenp6medPCdQtsf8z9Hc2s+IX+sWZJ86qRkfd/n04PhX/6d7yfrN005kqyjdWoe2c1srpltNLNXzGy3md2bLe8wsw1mti+7ndH8dgE0qp638ackrXb3BZKWSrrLzBZIekBSj7vPk9STPQbQpmqG3d373H1bdv+YpD2S5khaLmlt9rS1km5sUo8ASnBBn9nN7ApJiyRtljTL3fuy0iFJs3LW6ZLUJUkTNbnhRgEUU/fZeDObKul7ku5z97POCLm7S/KR1nP3bnfvdPfOcWrylQ8ActUVdjMbp6GgP+3uZ06/9pvZ7Kw+W9Lh5rQIoAw138abmUl6QtIed//asNJ6SSslPZzdPteUDktyYlr637W7Z+xr2r7v7m7etiVp1cFlubVFU99IrnvH9HS9nf32j25L1me+NLY1jYwS9Xxmv0bSrZJ2mtn2bNmDGgr5d83sDkmvS7q5KR0CKEXNsLv7i5LyZq5Pf1sEQNvg67JAEIQdCIKwA0EQdiAIwg4EEeYS14vZY3NerLqFhuwZGEjWBzx9LLriK+nt+5aXL7SlixpHdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IIsw4+/T/PpGsX/Xi7cn6jz7+eP62x0xsqKeLwUdfXpmsDw7mH0+uvOdQet3+Wr+Hkj8dNM7HkR0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgggzzj5207Zk/YpN6fWv+dJf5dZ23f7YhTfUIr/57F3JeseOvB8Ors/lT29J1v3UqdzaYKE940JxZAeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIMzd008wmyvpSUmzJLmkbnd/1MwekvQ5ST/Pnvqguz+f2tY06/AlxsSvQLNs9h4d9SMjfnmini/VnJK02t23mdklkraa2Yas9nV3/2pZjQJonnrmZ++T1JfdP2ZmeyTNaXZjAMp1QZ/ZzewKSYskbc4WrTKzHWa2xsxm5KzTZWa9ZtY7oPRPQwFonrrDbmZTJX1P0n3uflTS45KulLRQQ0f+R0Zaz9273b3T3TvHaULxjgE0pK6wm9k4DQX9aXf/viS5e7+7D7r7aUnflLS4eW0CKKpm2M3MJD0haY+7f23Y8tnDnnaTpF3ltwegLPWcjb9G0q2SdprZ9mzZg5JWmNlCDQ3HHZB0ZxP6A1CSes7GvyhppHG75Jg6gPbCN+iAIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANB1Pwp6VJ3ZvZzSa8PW3SppLdb1sCFadfe2rUvid4aVWZvv+ruHxyp0NKwn7dzs15376ysgYR27a1d+5LorVGt6o238UAQhB0Iouqwd1e8/5R27a1d+5LorVEt6a3Sz+wAWqfqIzuAFiHsQBCVhN3Mrjezn5rZfjN7oIoe8pjZATPbaWbbzay34l7WmNlhM9s1bFmHmW0ws33Z7Yhz7FXU20NmdjB77bab2Q0V9TbXzDaa2StmttvM7s2WV/raJfpqyevW8s/sZjZW0l5Jn5T0lqQtkla4+ystbSSHmR2Q1OnulX8Bw8x+T9JxSU+6+1XZsn+QdMTdH87+oZzh7ve3SW8PSTpe9TTe2WxFs4dPMy7pRkm3qcLXLtHXzWrB61bFkX2xpP3u/pq7n5T0rKTlFfTR9tz9BUlHzlm8XNLa7P5aDf3P0nI5vbUFd+9z923Z/WOSzkwzXulrl+irJaoI+xxJbw57/Jbaa753l/QDM9tqZl1VNzOCWe7el90/JGlWlc2MoOY03q10zjTjbfPaNTL9eVGcoDvfMne/WtKnJN2VvV1tSz70Gaydxk7rmsa7VUaYZvyXqnztGp3+vKgqwn5Q0txhjy/LlrUFdz+Y3R6WtE7tNxV1/5kZdLPbwxX380vtNI33SNOMqw1euyqnP68i7FskzTOzD5vZeEm3SFpfQR/nMbMp2YkTmdkUSdep/aaiXi9pZXZ/paTnKuzlLO0yjXfeNOOq+LWrfPpzd2/5n6QbNHRG/meSvlBFDzl9/Zqkn2R/u6vuTdIzGnpbN6Chcxt3SPqApB5J+yT9UFJHG/X2lKSdknZoKFizK+ptmYbeou+QtD37u6Hq1y7RV0teN74uCwTBCTogCMIOBEHYgSAIOxAEYQeCIOxAEIQdCOL/AXvXWz4vgv+wAAAAAElFTkSuQmCC\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plt.imshow(threes[3])\n",
    "plt.xticks()\n",
    "plt.yticks()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 251.565 248.518125\" width=\"251.565pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2022-03-03T15:51:37.641264</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.518125 \nL 251.565 248.518125 \nL 251.565 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 26.925 224.64 \nL 244.365 224.64 \nL 244.365 7.2 \nL 26.925 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p8272974c4b)\">\n    <image height=\"218\" id=\"image70b708f823\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"26.925\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAANoAAADaCAYAAADAHVzbAAAFDklEQVR4nO3dT2jXdRzHcX9uyVZm6qDaItqwrIhAEIn+LJCMiooYLaK6BWUdIoJAb906dC+oECqohNGhulbQIWkdikK0WmgHS0xdNNOU/M1unfy9dfv99tq/x+P62lc+l+c+8Pvym41tjdFzK4A5tXK+DwDLgdAgQGgQIDQIEBoECA0ChAYB3fN9gOVo4vXbyv3AyJvlfueL28t99dj4jM/E3HKjQYDQIEBoECA0CBAaBAgNAoQGAd6jzYNLJtv7/XZk5Ey5rx5r659nDrjRIEBoECA0CBAaBAgNAoQGAQ1/bm7hGdl3tNzXd/9d7rs2DnXyOHSAGw0ChAYBQoMAoUGA0CBAaBAgNAjwNZkFaPehLeX+1o3vl3vXDfeUe3PiwIzPRHvcaBAgNAgQGgQIDQKEBgFCgwChQYD3aIvQhu7ecv934IpyXznRydNwMdxoECA0CBAaBAgNAoQGAUKDAKFBgPdoC9BfnwzUP3BLPU9d11Pua2d2HDrAjQYBQoMAoUGA0CBAaBAgNAgQGgR4j7YA9e/+qdyP7/in3K99pv7C2Yn3Znwk2uRGgwChQYDQIEBoECA0CBAaBPh4fwFqHjte7i8feqDcb13ze7nvWbFqxmeiPW40CBAaBAgNAoQGAUKDAKFBgNAgQGgQIDQIEBoECA0ChAYBQoMAoUGA0CDA99EWob1H+8v97c2fl/vIzU+23Jr76z9Vx+y40SBAaBAgNAgQGgQIDQKEBgFCgwDv0Rah0+N95d69uavcT16/ruXWs39WR+IC3GgQIDQIEBoECA0ChAYBQoMAH+8vQoNjR8p96rnT5X7m+cmWW8+nszoSF+BGgwChQYDQIEBoECA0CBAaBAgNAhrbGqPn5vsQdNbwD/V7tBPNnpbb91vqV6vnzp6d1ZmWOzcaBAgNAoQGAUKDAKFBgNAgQGgQ4Ptoy9CrV37bcnu47/7y2eaRPzp9nGXBjQYBQoMAoUGA0CBAaBAgNAgQGgR4j7YEfT05VP9A377MQfifGw0ChAYBQoMAoUGA0CBAaBAgNAhYsu/Rjj17e7lfPnq43P881Vvuqz5eW+7r3/2m9TjdLJ9t15lXrq5/4IPW0y8vbSgfHdrp+2iz4UaDAKFBgNAgQGgQIDQIEBoELNmP9y89Nl3uLwx+Ue7DvfXH/31b6o//b7r36ZbbwIerymcv+/LHcm9OTZV7O5o9/hevueBGgwChQYDQIEBoECA0CBAaBAgNAhrbGqNenJzH9F2byn1yx6ly/2zTOy23NSt7ymc/Ormu3HfuGS33Rnf9DvHnrbtablv3Plo+23vfwXLn/NxoECA0CBAaBAgNAoQGAUKDAKFBgPdoc6RrY+s/2/brY1eVzw4/8l25v3HNV7M608U43KzfD26/4/FyP3vot04eZ8lwo0GA0CBAaBAgNAgQGgQIDQKEBgHeoy1EjUY5d20YLPeDT/WX+90Ptn5P90TfePnsaw/V31dr7p8o9+XKjQYBQoMAoUGA0CBAaBAgNAgQGgR4jwYBbjQIEBoECA0ChAYBQoMAoUGA0CBAaBAgNAgQGgQIDQKEBgFCgwChQYDQIEBoECA0CBAaBAgNAoQGAUKDAKFBgNAgQGgQIDQIEBoECA0ChAYBQoMAoUGA0CBAaBAgNAgQGgQIDQKEBgFCgwChQYDQIEBoECA0CBAaBAgNAoQGAUKDAKFBgNAgQGgQIDQIEBoECA0ChAYBQoOA/wD2fZFb5BBzuQAAAABJRU5ErkJggg==\" y=\"-6.64\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"mc98c31ac37\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.807857\" xlink:href=\"#mc98c31ac37\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(27.626607 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"69.636429\" xlink:href=\"#mc98c31ac37\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 5 -->\n      <g transform=\"translate(66.455179 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"108.465\" xlink:href=\"#mc98c31ac37\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 10 -->\n      <g transform=\"translate(102.1025 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"147.293571\" xlink:href=\"#mc98c31ac37\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 15 -->\n      <g transform=\"translate(140.931071 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"186.122143\" xlink:href=\"#mc98c31ac37\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 20 -->\n      <g transform=\"translate(179.759643 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"224.950714\" xlink:href=\"#mc98c31ac37\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 25 -->\n      <g transform=\"translate(218.588214 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m5bc0aeb93d\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m5bc0aeb93d\" y=\"11.082857\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0 -->\n      <g transform=\"translate(13.5625 14.882076)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m5bc0aeb93d\" y=\"49.911429\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 5 -->\n      <g transform=\"translate(13.5625 53.710647)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m5bc0aeb93d\" y=\"88.74\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 10 -->\n      <g transform=\"translate(7.2 92.539219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m5bc0aeb93d\" y=\"127.568571\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 15 -->\n      <g transform=\"translate(7.2 131.36779)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m5bc0aeb93d\" y=\"166.397143\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 20 -->\n      <g transform=\"translate(7.2 170.196362)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m5bc0aeb93d\" y=\"205.225714\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 25 -->\n      <g transform=\"translate(7.2 209.024933)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 26.925 224.64 \nL 26.925 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 244.365 224.64 \nL 244.365 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 26.925 224.64 \nL 244.365 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 26.925 7.2 \nL 244.365 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p8272974c4b\">\n   <rect height=\"217.44\" width=\"217.44\" x=\"26.925\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plt.imshow(sevens[3])\n",
    "plt.xticks()\n",
    "plt.yticks()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([6131, 28, 28]) torch.Size([6265, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(threes.shape, sevens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([12396, 28, 28])"
      ]
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "source": [
    "combined_data = torch.cat([threes, sevens])\n",
    "combined_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([12396, 784])"
      ]
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "source": [
    "flat_imgs = combined_data.view((-1, 28*28))\n",
    "flat_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([12396])"
      ]
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "source": [
    "target = torch.tensor([1]*len(threes)+[2]*len(sevens))\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x): \n",
    "    return 1/(1+torch.exp(-x))\n",
    "\n",
    "def simple_nn(data, weights, bias): \n",
    "    return sigmoid((data@weights) + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(pred, target): \n",
    "    return ((pred-target)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.randn((flat_imgs.shape[1], 1), requires_grad=True)\n",
    "b = torch.randn((1, 1), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss:  0.9550250172615051\n"
     ]
    }
   ],
   "source": [
    "for i in range(2000):\n",
    "    pred = simple_nn(flat_imgs, w, b)\n",
    "    loss = error(pred, target.unsqueeze(1))\n",
    "    loss.backward()\n",
    "    \n",
    "    w.data -= 0.001*w.grad.data\n",
    "    b.data -= 0.001*b.grad.data\n",
    "    \n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "\n",
    "print(\"Loss: \", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}